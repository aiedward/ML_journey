{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zHrlf9F-Uoj",
        "colab_type": "text"
      },
      "source": [
        "## Voting Classifier\n",
        "You can train your model using diverse algorithms and then ensemble them to predict the final output. Say, you use a Random Forest Classifier, SVM Classifier, Linear Regression etc.; models are pitted against each other and selected upon best performance by voting using the VotingClassifier Class from sklearn.ensemble.\n",
        "Hard voting is where a model is selected from an ensemble to make the final prediction by a simple majority vote for accuracy.\n",
        "Soft Voting can only be done when all your classifiers can calculate probabilities for the outcomes. Soft voting arrives at the best result by averaging out the probabilities calculated by individual algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPq_sOmR-bql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "rnd_clf = RandomForestClassifier()\n",
        "svm_clf = SVC()\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "\testimators = [('lr', log_clf), ('rf', rnd_clf), ('svc',svm_clf)],\n",
        "\tvoting = 'hard')\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSbqSQE-cVu",
        "colab_type": "text"
      },
      "source": [
        "## Bagging\n",
        "nstead of running various models on a single dataset, you can use a single model over various random subsets of the dataset. Random sampling with replacement is called Bagging, short for bootstrap aggregating. In case that was difficult to visualize in your head, just imagine disregarding several random entries in the dataset and modelling with the rest. In case of Pasting, the same process applies, only difference being that pasting doesnâ€™t allow training instances to be sampled several times for the same predictors.\n",
        "Code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvpFZ-8a-eL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAC2xj46-iQJ",
        "colab_type": "text"
      },
      "source": [
        "## Out of the box evaluation\n",
        "When performing Bagging on a training set, only 63% of the instances are included in the model, that means there are 37% of the instances that the classifier has not seen before. These can be used for evaluation just like Cross-Validation.\n",
        "To use this functionality, simply add a oob_score = True parameter in the BaggingClassifier class in the prior example.\n",
        "Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gMx2QgX-n3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "max_samples=100, bootstrap=True, n_jobs=-1, random_state=42,\n",
        "oob_score = True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pISiWKe-9ys",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest is also a example of Ensemble Learning"
      ]
    }
  ]
}